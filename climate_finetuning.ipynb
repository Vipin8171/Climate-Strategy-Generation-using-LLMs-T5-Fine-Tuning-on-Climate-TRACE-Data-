{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc22e2c0-4300-49f6-b031-2762ae388a68",
   "metadata": {},
   "source": [
    "# Emission Reduction Strategy Generator (T5-small fine-tune)\n",
    "This notebook fine-tunes a small seq2seq model (T5-small) on synthetic climate/emission strategy examples\n",
    "and demonstrates inference and simple evaluation (ROUGE / BLEU). All steps are local and runnable in Jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71520ec0-9568-420f-8f09-2891b53aaa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fdd2c632-1b58-4771-a9f3-dd03cd3dfe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 40000  Validation: 5000  Test: 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facility: Šilalės District Municipality | Country: LTU | Sector: biological-treatment-of-solid-waste-and-biogenic | EmissionReductionPotential(Mt): 0.000 | Difficulty: Long-term</td>\n",
       "      <td>Flare CH4  from anaerobic digestion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facility: Ripiceni Commune | Country: ROU | Sector: manure-left-on-pasture-cattle | EmissionReductionPotential(Mt): 0.000 | Difficulty: Long-term</td>\n",
       "      <td>Rotating herds between multiple pastures allows pastures to recover between seasons and increase carbon stored in the soil and improves feed efficiency. In addition, growing specific fodder crops like plaintain could also reduce N2O.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facility: Nhommalath District | Country: LAO | Sector: forest-land-fires | EmissionReductionPotential(Mt): 0.309 | Difficulty: Long-term</td>\n",
       "      <td>Fire risk mitigation includes: Fuel load reduction, fire-resilient forest management, landscape-level fire planning and zoning, restoration of fire-adaptive ecosystems, and post-fire recovery.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facility: ITA_MatureDairyCattle_10230 | Country: ITA | Sector: manure-management-cattle-operation | EmissionReductionPotential(Mt): 0.000 | Difficulty: Long-term</td>\n",
       "      <td>For dairy cattle high productivity, change manure handling to solid storage: where manure is stored, typically for several months, in unconfined piles or stacks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Facility: POL_MatureDairyCattle_9543 | Country: POL | Sector: manure-management-cattle-operation | EmissionReductionPotential(Mt): 0.000 | Difficulty: Long-term</td>\n",
       "      <td>For dairy cattle high productivity, modify the existing system and change manure handling to dry lot, where the manure is periodically removed from the paved or unpaved confined area and can be spread onto fields.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                               input  \\\n",
       "0  Facility: Šilalės District Municipality | Country: LTU | Sector: biological-treatment-of-solid-waste-and-biogenic | EmissionReductionPotential(Mt): 0.000 | Difficulty: Long-term   \n",
       "1                                  Facility: Ripiceni Commune | Country: ROU | Sector: manure-left-on-pasture-cattle | EmissionReductionPotential(Mt): 0.000 | Difficulty: Long-term   \n",
       "2                                           Facility: Nhommalath District | Country: LAO | Sector: forest-land-fires | EmissionReductionPotential(Mt): 0.309 | Difficulty: Long-term   \n",
       "3                  Facility: ITA_MatureDairyCattle_10230 | Country: ITA | Sector: manure-management-cattle-operation | EmissionReductionPotential(Mt): 0.000 | Difficulty: Long-term   \n",
       "4                   Facility: POL_MatureDairyCattle_9543 | Country: POL | Sector: manure-management-cattle-operation | EmissionReductionPotential(Mt): 0.000 | Difficulty: Long-term   \n",
       "\n",
       "                                                                                                                                                                                                                                      output  \n",
       "0                                                                                                                                                                                                        Flare CH4  from anaerobic digestion  \n",
       "1  Rotating herds between multiple pastures allows pastures to recover between seasons and increase carbon stored in the soil and improves feed efficiency. In addition, growing specific fodder crops like plaintain could also reduce N2O.  \n",
       "2                                           Fire risk mitigation includes: Fuel load reduction, fire-resilient forest management, landscape-level fire planning and zoning, restoration of fire-adaptive ecosystems, and post-fire recovery.  \n",
       "3                                                                          For dairy cattle high productivity, change manure handling to solid storage: where manure is stored, typically for several months, in unconfined piles or stacks.  \n",
       "4                      For dairy cattle high productivity, modify the existing system and change manure handling to dry lot, where the manure is periodically removed from the paved or unpaved confined area and can be spread onto fields.  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"processed_data_50k/train.csv\")\n",
    "val_df   = pd.read_csv(\"processed_data_50k/val.csv\")\n",
    "test_df  = pd.read_csv(\"processed_data_50k/test.csv\")\n",
    "\n",
    "print(\"Train:\", len(train_df), \" Validation:\", len(val_df), \" Test:\", len(test_df))\n",
    "\n",
    "# preview\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7de7c49d-ad70-4c1e-b488-deba83d198e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4 — Convert Pandas → HuggingFace Dataset Format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset   = Dataset.from_pandas(val_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "967d65e5-688e-4d76-b05d-3d94670048d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4846ac75b684a55be0a6382ff655fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tvipi\\.cache\\huggingface\\hub\\models--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0e2052c9a5451ea5d0153e3736bff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b442a55a804f9583e6ada758d25881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae47260a6fe74d4281410412a4979e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b007a205634c94a9dac400488b67ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load Tokenizer & Model (T5-Small)\n",
    "model_name = \"t5-base\"\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "max_input_len = 256\n",
    "max_output_len = 180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "885931cc-29b4-449e-8022-166a0f7e74c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafe766c9822413696c09ef7732bb2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tvipi\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c148396d131d4ba0bb599be9225af8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7324c0690b094421ab564efa24ef24c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization Function\n",
    "def tokenize_fn(batch):\n",
    "    inputs = [\"generate strategy: \" + x for x in batch[\"input\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # tokenize strategy descriptions (targets)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"output\"],\n",
    "            max_length=max_output_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"output\"]\n",
    ")\n",
    "\n",
    "tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "21e7a80a-407d-4eab-9663-aa21a47b876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator & Training Arguments\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5_base_climatetrace_finetuned_50k\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,  # increase to 4–6 if using GPU\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "061d60da-bdad-4e4a-bcd1-97babb9ad7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics (ROUGE + BLEU)\n",
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu  = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Decode labels (replace -100)\n",
    "    labels = [\n",
    "        [(l if l != -100 else tokenizer.pad_token_id) for l in label]\n",
    "        for label in labels\n",
    "    ]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Clean \n",
    "    decoded_preds = [p.strip() for p in decoded_preds]\n",
    "    decoded_labels = [l.strip() for l in decoded_labels]\n",
    "\n",
    "    # ---- ROUGE (now returns floats directly) ----\n",
    "    rouge_scores = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )\n",
    "\n",
    "    # ---- BLEU (correct format: list of lists) ----\n",
    "    bleu_scores = bleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[ref] for ref in decoded_labels]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"],\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"],\n",
    "        \"bleu\": bleu_scores[\"score\"]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fe1ebe4d-3333-49f8-80af-461a7e4a4971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tvipi\\AppData\\Local\\Temp\\ipykernel_6656\\1160634596.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9486c1c-6624-4be9-b375-a492cd3057b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1089' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1089/25000 52:13 < 19:08:45, 0.35 it/s, Epoch 0.22/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6c8c9-05b6-4195-95f4-ef5e7378ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/t5_climate_strategy_final\")\n",
    "tokenizer.save_pretrained(\"models/t5_climate_strategy_final\")\n",
    "\n",
    "print(\"Model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821abcd4-ea4b-41db-8b46-01d1da829dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Helper\n",
    "def generate_strategy(text, max_len=180, num_beams=4):\n",
    "    inp = \"generate strategy: \" + text\n",
    "    tokens = tokenizer(inp, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    gen_ids = model.generate(\n",
    "        **tokens,\n",
    "        max_length=max_len,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fb9ea-68bb-4a8f-9a9c-2b1a7ce5cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Real Examples\n",
    "examples = [\n",
    "    \"Facility: Sunndalsora aluminium plant | Country: NOR | Sector: aluminum | EmissionReductionPotential(Mt): 0.78 | Difficulty: Short-term\",\n",
    "    \"Facility: Some Cement Plant | Country: IND | Sector: cement | EmissionReductionPotential(Mt): 1.52 | Difficulty: Mid-term\"\n",
    "]\n",
    "\n",
    "for e in examples:\n",
    "    print(\"INPUT:\", e)\n",
    "    print(\"OUTPUT:\", generate_strategy(e))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efb525-bd33-4279-8380-e5ba86508e3c",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1003330-1d0c-4d6b-8beb-4936b356c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "refs = []\n",
    "\n",
    "for row in test_df.to_dict(\"records\")[:50]:  # evaluate first 50\n",
    "    p = generate_strategy(row[\"input\"])\n",
    "    preds.append(p)\n",
    "    refs.append(row[\"output\"])\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "for i in range(3):\n",
    "    print(\"\\nInput:\", test_df.iloc[i][\"input\"])\n",
    "    print(\"Pred:\", preds[i])\n",
    "    print(\"True:\", refs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7c3b6-3e15-46b5-b8aa-88bf18c6d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyer import notification\n",
    "\n",
    "notification.notify(\n",
    "    title='Jupyter Notebook',\n",
    "    message='✅ All cells finished running successfully!',\n",
    "    timeout=10  # seconds\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63526ae1-3bd0-43ce-b453-aba73ca31013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD MODELS\n",
    "# -------------------------------\n",
    "\n",
    "# Fine-tuned model\n",
    "ft_model_path = \"models/t5_climate_strategy_final\"\n",
    "ft_tokenizer = T5TokenizerFast.from_pretrained(ft_model_path)\n",
    "ft_model = T5ForConditionalGeneration.from_pretrained(ft_model_path).to(device)\n",
    "\n",
    "# Base T5 model\n",
    "base_model_name = \"t5-small\"\n",
    "base_tokenizer = T5TokenizerFast.from_pretrained(base_model_name)\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(base_model_name).to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# Load test dataset and sample 2 rows\n",
    "# -------------------------------\n",
    "test_df = pd.read_csv(\"processed_data/test.csv\")\n",
    "examples = test_df.sample(2).to_dict(\"records\")\n",
    "\n",
    "print(\"Selected Inputs:\\n\")\n",
    "for i, ex in enumerate(examples, 1):\n",
    "    print(f\"{i}. {ex['input']}\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# Helper function for generation\n",
    "# -------------------------------\n",
    "def generate(model, tokenizer, text, max_len=200):\n",
    "    encoded = tokenizer(\n",
    "        \"generate strategy: \" + text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **encoded,\n",
    "        max_length=max_len,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate both models on both inputs\n",
    "# -------------------------------\n",
    "\n",
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu  = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for ex in examples:\n",
    "    inp = ex[\"input\"]\n",
    "    true = ex[\"output\"]\n",
    "\n",
    "    base_pred = generate(base_model, base_tokenizer, inp)\n",
    "    ft_pred   = generate(ft_model, ft_tokenizer, inp)\n",
    "\n",
    "    # --- Compute ROUGE ---\n",
    "    base_rouge = rouge.compute(predictions=[base_pred], references=[true])\n",
    "    ft_rouge   = rouge.compute(predictions=[ft_pred], references=[true])\n",
    "\n",
    "    # --- Compute BLEU ---\n",
    "    base_bleu = bleu.compute(predictions=[base_pred], references=[[true]])[\"score\"]\n",
    "    ft_bleu   = bleu.compute(predictions=[ft_pred], references=[[true]])[\"score\"]\n",
    "\n",
    "    results.append({\n",
    "        \"Input\": inp,\n",
    "        \"Ground Truth\": true,\n",
    "        \n",
    "        \"Base Prediction\": base_pred,\n",
    "        \"FT Prediction\": ft_pred,\n",
    "\n",
    "        \"Base ROUGE-1\": base_rouge[\"rouge1\"],\n",
    "        \"Base ROUGE-L\": base_rouge[\"rougeL\"],\n",
    "        \"Base BLEU\": base_bleu,\n",
    "\n",
    "        \"FT ROUGE-1\": ft_rouge[\"rouge1\"],\n",
    "        \"FT ROUGE-L\": ft_rouge[\"rougeL\"],\n",
    "        \"FT BLEU\": ft_bleu\n",
    "    })\n",
    "\n",
    "# -------------------------------\n",
    "# Create a beautiful comparison table\n",
    "# -------------------------------\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "comparison_df = pd.DataFrame(results)\n",
    "\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b44d3b-123f-4377-b6bc-0391805e7c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu_env]",
   "language": "python",
   "name": "conda-env-gpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
